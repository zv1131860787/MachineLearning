以下是一个使用K-Means算法进行聚类的完整计算样例，假设有一个二维数据集，我们通过手动计算的方式来展示算法流程：

### 1. 数据集准备
假设有如下包含6个二维数据点的数据集 $X$：

| 数据点编号 | 坐标（$x$值，$y$值） |
|:-----------:|:-------------------:|
| 1           | (1, 1)              |
| 2           | (1.5, 2)            |
| 3           | (3, 4)              |
| 4           | (5, 7)              |
| 5           | (3.5, 5)            |
| 6           | (4, 5)              |

我们设定聚类的数量 $k = 2$。

### 2. 初始化聚类中心
随机选择两个数据点作为初始聚类中心，这里我们选择数据点1（坐标为(1, 1) ）作为第一个聚类中心 $\mu_1$，数据点4（坐标为(5, 7) ）作为第二个聚类中心 $\mu_2$。

### 3. 分配数据点到聚类（第一轮迭代）
使用欧几里得距离公式$d(x,y)=\sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$（这里 $n = 2$，对应二维坐标的两个维度）来计算每个数据点到两个聚类中心的距离，并将其分配到距离最近的聚类中心对应的类中。

- 对于数据点1（坐标为(1, 1) ）：
  - 到 $\mu_1=(1, 1)$ 的距离 $d_1 = \sqrt{(1 - 1)^2 + (1 - 1)^2} = 0$
  - 到 $\mu_2=(5, 7)$ 的距离 $d_2 = \sqrt{(1 - 5)^2 + (1 - 7)^2}=\sqrt{16 + 36}=\sqrt{52} \approx 7.21$
  因为 $d_1 < d_2$，所以数据点1分配到聚类1中。

- 对于数据点2（坐标为(1.5, 2) ）：
  - 到 $\mu_1=(1, 1)$ 的距离 $d_1 = \sqrt{(1.5 - 1)^2 + (2 - 1)^2}=\sqrt{0.25 + 1}=\sqrt{1.25} \approx 1.12$
  - 到 $\mu_2=(5, 7)$ 的距离 $d_2 = \sqrt{(1.5 - 5)^2 + (2 - 7)^2}=\sqrt{12.25 + 25}=\sqrt{37.25} \approx 6.10$
  因为 $d_1 < d_2$，所以数据点2分配到聚类1中。

- 对于数据点3（坐标为(3, 4) ）：
  - 到 $\mu_1=(1, 1)$ 的距离 $d_1 = \sqrt{(3 - 1)^2 + (4 - 1)^2}=\sqrt{4 + 9}=\sqrt{13} \approx 3.61$
  - 到 $\mu_2=(5, 7)$ 的距离 $d_2 = \sqrt{(3 - 5)^2 + (4 - 7)^2}=\sqrt{4 + 9}=\sqrt{13} \approx 3.61$
  这里我们按照规则（比如优先分配到索引小的聚类中心对应的类等，需提前确定好规则），将数据点3分配到聚类1中。

- 对于数据点4（坐标为(5, 7) ）：
  - 到 $\mu_1=(1, 1)$ 的距离 $d_1 = \sqrt{(5 - 1)^2 + (7 - 1)^2}=\sqrt{16 + 36}=\sqrt{52} \approx 7.21$
  - 到 $\mu_2=(5, 7)$ 的距离 $d_2 = \sqrt{(5 - 5)^2 + (7 - 7)^2} = 0$
  所以数据点4分配到聚类2中。

- 对于数据点5（坐标为(3.5, 5) ）：
  - 到 $\mu_1=(1, 1)$ 的距离 $d_1 = \sqrt{(3.5 - 1)^2 + (5 - 1)^2}=\sqrt{6.25 + 16}=\sqrt{22.25} \approx 4.72$
  - 到 $\mu_2=(5, 7)$ 的距离 $d_2 = \sqrt{(3.5 - 5)^2 + (5 - 7)^2}=\sqrt{2.25 + 4}=\sqrt{6.25} \approx 2.50$
  所以数据点5分配到聚类2中。

- 对于数据点6（坐标为(4, 5) ）：
  - 到 $\mu_1=(1, 1)$ 的距离 $d_1 = \sqrt{(4 - 1)^2 + (5 - 1)^2}=\sqrt{9 + 16}=\sqrt{25} = 5$
  - 到 $\mu_2=(5, 7)$ 的距离 $d_2 = \sqrt{(4 - 5)^2 + (5 - 7)^2}=\sqrt{1 + 4}=\sqrt{5} \approx 2.24$
  所以数据点6分配到聚类2中。

第一轮迭代后，聚类1包含数据点1、2、3，聚类2包含数据点4、5、6。

### 4. 更新聚类中心（第一轮迭代后）
- 对于聚类1：
  数据点为 (1, 1)、(1.5, 2)、(3, 4)，新的聚类中心 $\mu_1$ 的坐标计算如下：
  $x$坐标：$(1 + 1.5 + 3) / 3 = 1.83$（保留两位小数）
  $y$坐标：$(1 + 2 + 4) / 3 = 2.33$（保留两位小数）
  所以更新后的 $\mu_1=(1.83, 2.33)$。

- 对于聚类2：
  数据点为 (5, 7)、(3.5, 5)、(4, 5)，新的聚类中心 $\mu_2$ 的坐标计算如下：
  $x$坐标：$(5 + 3.5 + 4) / 3 = 4.17$（保留两位小数）
  $y$坐标：$(7 + 5 + 5) / 3 = 5.67$（保留两位小数）
  所以更新后的 $\mu_2=(4.17, 5.67)$。

### 5. 分配数据点到聚类（第二轮迭代）
再次计算每个数据点到更新后的两个聚类中心的距离，并重新分配。

- 对于数据点1（坐标为(1, 1) ）：
  - 到 $\mu_1=(1.83, 2.33)$ 的距离 $d_1 = \sqrt{(1 - 1.83)^2 + (1 - 2.33)^2}=\sqrt{0.6889 + 1.7689}=\sqrt{2.4578} \approx 1.57$
  - 到 $\mu_2=(4.17, 5.67)$ 的距离 $d_2 = \sqrt{(1 - 4.17)^2 + (1 - 5.67)^2}=\sqrt{9.9889 + 21.8089}=\sqrt{31.7978} \approx 5.64$
  因为 $d_1 < d_2$，所以数据点1仍分配到聚类1中。

（依次类似地计算其他数据点到新聚类中心的距离并分配，过程略）

经过第二轮迭代后，发现数据点的分配情况和第一轮迭代后没有变化（实际情况中可能还需要继续迭代，直到聚类中心不再变化或者达到设定的最大迭代次数）。

### 6. 最终聚类结果
最终得到两个聚类：
- 聚类1包含数据点1、2、3，聚类中心为(1.83, 2.33)。
- 聚类2包含数据点4、5、6，聚类中心为(4.17, 5.67)。

这样就通过K-Means算法完成了对给定数据集的聚类操作，将数据点划分为了两个不同的类别。在实际应用中，数据集往往规模更大，维度更多，通常会借助编程工具（如Python的`sklearn`库等）来高效地实现K-Means算法的计算过程。 


